{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bank Note Authentication Dataset\n",
    "\n",
    "The dataset contains following four input features computed from 400x400 images of bank notes:\n",
    "1. variance of Wavelet Transformed image (continuous)\n",
    "2. skewness of Wavelet Transformed image (continuous)\n",
    "3. Curtosis of Wavelet Transformed image (continuous)\n",
    "4. entropy of image (continuous)\n",
    "Each instance is labelled as fake (label 0) or authentic (label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given dataset, implement Decision Tree-based CART algorithm:\n",
    "\n",
    "# (i) Inbuilt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      variance  skewness  curtosis  entropy  Target\n",
      "0      3.62160   8.66610   -2.8073 -0.44699       0\n",
      "1      4.54590   8.16740   -2.4586 -1.46210       0\n",
      "2      3.86600  -2.63830    1.9242  0.10645       0\n",
      "3      3.45660   9.52280   -4.0112 -3.59440       0\n",
      "4      0.32924  -4.45520    4.5718 -0.98880       0\n",
      "...        ...       ...       ...      ...     ...\n",
      "1367   0.40614   1.34920   -1.4501 -0.55949       1\n",
      "1368  -1.38870  -4.87730    6.4774  0.34179       1\n",
      "1369  -3.75030 -13.45860   17.5932 -2.77710       1\n",
      "1370  -3.56370  -8.38270   12.3930 -1.28230       1\n",
      "1371  -2.54190  -0.65804    2.6842  1.19520       1\n",
      "\n",
      "[1372 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bank_notes.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190   1]\n",
      " [  4 148]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9854227405247813"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii) Step-By-Step Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "\n",
    "# function to calculate entropy of a complete set\n",
    "def find_entropy(df) :\n",
    "  Class = df.keys()[-1]\n",
    "  entropy = 0\n",
    "  values = df[Class].unique()\n",
    "  for value in values :\n",
    "    fraction = df[Class].value_counts()[value]/len(df[Class]) # number of instances of this value divided by the total number of instances\n",
    "    entropy  = entropy + -fraction*np.log2(fraction)\n",
    "  return entropy\n",
    "\n",
    "# function to calculate the entropy of a feature given a set\n",
    "def find_entropy_attribute(df,attribute) :\n",
    "  Class = df.keys()[-1]\n",
    "  target_variables = df[Class].unique()\n",
    "  variables = df[attribute].unique()\n",
    "  entropy2 =0 \n",
    "  for variable in variables :\n",
    "    entropy = 0\n",
    "    for target_variable in target_variables :\n",
    "      num = len(df[attribute][df[attribute]==variable][df[Class]==target_variable])\n",
    "      den = len(df[attribute][df[attribute]==variable])\n",
    "      fraction = num/(den+eps)\n",
    "      entropy += -fraction*log(fraction+eps)\n",
    "    fraction2 = den/len(df)\n",
    "    entropy2 += -fraction2*entropy\n",
    "  return abs(entropy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the feature with the highest information gain\n",
    "def find_winner(df) :\n",
    "  IG = []\n",
    "  for key in df.keys()[:-1] :\n",
    "    IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
    "  return df.keys()[:-1][np.argmax(IG)]\n",
    "  \n",
    "# function that returns the subtable that meet the given condition\n",
    "def get_subtable(df,node,value) :\n",
    "  return df[df[node]==value].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing decision tree\n",
    "def buildTree(df,tree=None) :\n",
    "  Class = df.keys()[-1]\n",
    "  node = find_winner(df) # node is the attribute with the maximum information gain\n",
    "  attValue = np.unique(df[node]) # returns distinct values for that attribute\n",
    "  # empty dictionary for the tree\n",
    "  if tree is None :\n",
    "    tree = {}\n",
    "    tree[node] = {}\n",
    "  # the tree is made recursively for subsets of the data anytime a subset is pure the loop stops\n",
    "  for value in attValue :\n",
    "    subTable = get_subtable(df,node,value)\n",
    "    c1Value,counts = np.unique(subTable['Target'],return_counts=True)\n",
    "    if len(counts)==1: # checking purity of the subset\n",
    "      tree[node][value] = c1Value[0]\n",
    "    else :\n",
    "      tree[node][value] = buildTree(subTable)\n",
    "  return tree\n",
    "tree = buildTree(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix [[372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       372\n",
      "\n",
      "    accuracy                           1.00       372\n",
      "   macro avg       1.00      1.00      1.00       372\n",
      "weighted avg       1.00      1.00      1.00       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(inst,tree) :\n",
    "  for nodes in tree.keys() :\n",
    "    value = inst[nodes]\n",
    "    tree = tree[nodes][value]\n",
    "    prediction = 0\n",
    "    if type(tree) is dict :\n",
    "      prediction = predict(inst,tree)\n",
    "    else :\n",
    "      prediction = tree\n",
    "      break\n",
    "  return prediction\n",
    "\n",
    "df1 =df.iloc[1000:,]\n",
    "Y_test = df.iloc[1000:,-1]\n",
    "Y_label = []\n",
    "for i in range(len(df1)) :\n",
    "  inst = df1.iloc[i,:]\n",
    "  prediction = predict(inst,tree)\n",
    "  Y_label.append(prediction)\n",
    "\n",
    "print(\"Confusion Matrix\",metrics.confusion_matrix(Y_test,Y_label))\n",
    "print(metrics.classification_report(Y_test,Y_label))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79ac8789342556743643626decb7769411d3fd989826a457f58a99a538c5aed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
